{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFfGR46OpruC",
        "outputId": "153d7b93-ed96-4289-eff8-2807410a2c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DEbrxTgpbJ_"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "# Libraries | Utils | Settings\n",
        "################################\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "from scipy.io import loadmat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r '/content/drive/My Drive/Colab Notebooks/data' '/tmp/data'"
      ],
      "metadata": {
        "id": "Lp0zHlAgm7Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class LoadMixedData(Dataset):\n",
        "    def __init__(self, size, train = 'True', train_test_split = 0.9):\n",
        "\n",
        "        super(LoadMixedData, self).__init__()\n",
        "\n",
        "        self.size = size\n",
        "        self.inputs = []\n",
        "        self.targets = set()\n",
        "\n",
        "        datasets = [\"AFLW2000\", \"afw\", \"helen_trainset\",\n",
        "                \"ibug\", \"lfpw_1\", \"lfpw_2\", \"synthetic1000\"]\n",
        "        # reserve \"helen_testset\" for final evaluation\n",
        "\n",
        "        for dataset in datasets:\n",
        "            data_dir = \"/tmp/data/\" + dataset\n",
        "            for fname in os.listdir(data_dir):\n",
        "                if fname.endswith(\"seg.png\"):\n",
        "                    continue\n",
        "                if fname.endswith(\"jpg\") or fname.endswith(\"png\"):\n",
        "                    self.inputs.append(os.path.join(data_dir, fname))\n",
        "                if fname.endswith(\"mat\") or fname.endswith(\"ldmks.txt\") or fname.endswith(\"pts\"):\n",
        "                    self.targets.add(os.path.join(data_dir, fname))\n",
        "\n",
        "        self.inputs.sort()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        load image and mask from index idx of your data\n",
        "        '''\n",
        "        input_dir = self.inputs[idx]\n",
        "\n",
        "        # load original image\n",
        "        img = cv2.imread(input_dir)\n",
        "        original_h, original_w, channel = img.shape\n",
        "        # load original labels\n",
        "        ls = input_dir.split(\"/\")\n",
        "        fname = ls.pop().split(\".\")[0]\n",
        "        ls.append(fname)\n",
        "        target_dir = os.path.join(\"/\", *ls)\n",
        "        if target_dir.endswith(\"mirror\"):\n",
        "            target_dir = target_dir[:-7] + \".pts\"\n",
        "            with open(target_dir) as f:\n",
        "                rows = [rows.strip() for rows in f]\n",
        "            head = rows.index('{') + 1\n",
        "            tail = rows.index('}')\n",
        "            raw_points = rows[head:tail]\n",
        "            ldmks = [[],[]]\n",
        "            for point in raw_points:\n",
        "                x, y = list(map(float, point.split()))\n",
        "                ldmks[0].append(original_w - x)\n",
        "                ldmks[1].append(y)\n",
        "            ldmks = np.array(ldmks)\n",
        "        elif target_dir + \".mat\" in self.targets:\n",
        "            target_dir = target_dir + \".mat\"\n",
        "            ldmks = np.array(loadmat(target_dir)['pt3d_68']) # (3, 68)\n",
        "            ldmks = ldmks[:2, :]\n",
        "        elif target_dir + \"_ldmks.txt\" in self.targets:\n",
        "            target_dir = target_dir + \"_ldmks.txt\"\n",
        "            ldmks = [[],[]]\n",
        "            with open(target_dir, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "            for i, line in enumerate(lines):\n",
        "                x, y = list(map(float, line.split(\" \")))\n",
        "                ldmks[0].append(x)\n",
        "                ldmks[1].append(y)\n",
        "            ldmks[0] = ldmks[0][:68]\n",
        "            ldmks[1] = ldmks[1][:68]\n",
        "            ldmks = np.array(ldmks)\n",
        "        else:\n",
        "            target_dir = target_dir + \".pts\"\n",
        "            with open(target_dir) as f:\n",
        "                rows = [rows.strip() for rows in f]\n",
        "            head = rows.index('{') + 1\n",
        "            tail = rows.index('}')\n",
        "            raw_points = rows[head:tail]\n",
        "            ldmks = [[],[]]\n",
        "            for point in raw_points:\n",
        "                x, y = list(map(float, point.split()))\n",
        "                ldmks[0].append(x)\n",
        "                ldmks[1].append(y)\n",
        "            ldmks = np.array(ldmks)\n",
        "\n",
        "        # crop image and adjust labels to match\n",
        "        center_x, center_y = np.mean(ldmks[0, :]), np.mean(ldmks[1, :])\n",
        "        span_x, span_y = np.max(ldmks[0, :]) - np.min(ldmks[0, :]), np.max(ldmks[1, :]) - np.min(ldmks[1, :])\n",
        "        xmin, xmax = max(0, center_x - span_x), min(original_w, center_x + span_x)\n",
        "        ymin, ymax = max(0, center_y - span_y), min(original_h, center_y + span_y)\n",
        "        img = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
        "        crop_h, crop_w, channels = img.shape\n",
        "        ldmks[0, :] -= xmin\n",
        "        ldmks[1, :] -= ymin\n",
        "\n",
        "        # resize image and adjust labels to match\n",
        "        ratio_x = self.size / crop_w\n",
        "        ratio_y = self.size / crop_h\n",
        "        img = cv2.resize(img, (self.size, self.size))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        ldmks[0, :] *= ratio_x\n",
        "        ldmks[1, :] *= ratio_y\n",
        "\n",
        "        #return image and mask in tensors\n",
        "        image = torch.from_numpy(img)\n",
        "        image = image.permute(2, 0, 1) # (192 * 192 *3) -> (3 * 192 * 192)\n",
        "        ldmks = torch.from_numpy(ldmks)\n",
        "\n",
        "        return image, ldmks\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n"
      ],
      "metadata": {
        "id": "kP02uZ63pyjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# FaceMeshBlock\n",
        "#################################\n",
        "# This is the main building block for FaceMesh architecture\n",
        "\n",
        "class FaceMeshBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
        "        super(FaceMeshBlock, self).__init__()\n",
        "\n",
        "        self.stride = stride\n",
        "        self.channel_pad = out_channels - in_channels\n",
        "\n",
        "        # TFLite uses slightly different padding than PyTorch\n",
        "        # on the depthwise conv layer when the stride is 2.\n",
        "        if stride == 2:\n",
        "            self.max_pool = nn.MaxPool2d(kernel_size=stride, stride=stride)\n",
        "            padding = 0\n",
        "        else:\n",
        "            padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                groups=in_channels,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.BatchNorm2d(\n",
        "                in_channels\n",
        "                ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.BatchNorm2d(\n",
        "                out_channels\n",
        "                )\n",
        "        )\n",
        "\n",
        "        self.act = nn.PReLU(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.stride == 2:\n",
        "            h = F.pad(x, (0, 2, 0, 2), \"constant\", 0)\n",
        "            x = self.max_pool(x)\n",
        "        else:\n",
        "            h = x\n",
        "\n",
        "        if self.channel_pad > 0:\n",
        "            x = F.pad(x, (0, 0, 0, 0, 0, self.channel_pad), \"constant\", 0)\n",
        "\n",
        "        return self.act(self.convs(h) + x)\n",
        "\n",
        "#########################################\n",
        "#       FaceMesh\n",
        "#########################################\n",
        "\n",
        "class FaceMesh(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FaceMesh, self).__init__()\n",
        "\n",
        "        self.num_coords = 68\n",
        "        self.x_scale = 192.0\n",
        "        self.y_scale = 192.0\n",
        "        self.min_score_thresh = 0.75\n",
        "\n",
        "        self._define_layers()\n",
        "\n",
        "    def _define_layers(self):\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.PReLU(16),\n",
        "\n",
        "            FaceMeshBlock(16, 16),\n",
        "            FaceMeshBlock(16, 16),\n",
        "            FaceMeshBlock(16, 32, stride=2),\n",
        "            FaceMeshBlock(32, 32),\n",
        "            FaceMeshBlock(32, 32),\n",
        "            FaceMeshBlock(32, 64, stride=2),\n",
        "            FaceMeshBlock(64, 64),\n",
        "            FaceMeshBlock(64, 64),\n",
        "            FaceMeshBlock(64, 128, stride=2),\n",
        "            FaceMeshBlock(128, 128),\n",
        "            FaceMeshBlock(128, 128),\n",
        "            FaceMeshBlock(128, 128, stride=2),\n",
        "            FaceMeshBlock(128, 128),\n",
        "            FaceMeshBlock(128, 128),\n",
        "        )\n",
        "\n",
        "        self.coord_head = nn.Sequential(\n",
        "            FaceMeshBlock(128, 128, stride=2),\n",
        "            FaceMeshBlock(128, 128),\n",
        "            FaceMeshBlock(128, 128),\n",
        "            nn.Conv2d(128, 32, 1),\n",
        "            nn.PReLU(32),\n",
        "            FaceMeshBlock(32, 32),\n",
        "            nn.Conv2d(32, 3 * self.num_coords, 3)\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            FaceMeshBlock(128, 128, stride=2),\n",
        "            nn.Conv2d(128, 32, 1),\n",
        "            nn.PReLU(32),\n",
        "            FaceMeshBlock(32, 32),\n",
        "            nn.Conv2d(32, 1, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ConstantPad2d((1, 0, 1, 0), 0)(x)\n",
        "        b = x.shape[0]      # batch size, needed for reshaping later\n",
        "\n",
        "        x = self.backbone(x)            # (b, 128, 6, 6)\n",
        "\n",
        "        c = self.conf_head(x)           # (b, 1, 1, 1)\n",
        "        c = c.view(b, -1)               # (b, 1)\n",
        "\n",
        "        r = self.coord_head(x)          # (b, 3 * self.num_coords, 1, 1)\n",
        "        r = r.reshape(b, -1)            # (b, 3 * self.num_coords)\n",
        "\n",
        "        return [r, c]\n",
        "\n",
        "    def _device(self):\n",
        "        return self.conf_head[1].weight.device\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        self.eval()\n",
        "\n",
        "    def _preprocess(self, x):\n",
        "        \"\"\"Converts the image pixels to the range [-1, 1].\"\"\"\n",
        "        return x.float() / 127.5 - 1.0\n",
        "\n",
        "    def predict_on_batch(self, x):\n",
        "        \"\"\"Makes a prediction on a batch of images.\n",
        "\n",
        "        Arguments:\n",
        "            x: a NumPy array of shape (b, H, W, 3) or a PyTorch tensor of\n",
        "               shape (b, 3, H, W). The height and width should be 128 pixels.\n",
        "\n",
        "        Returns:\n",
        "            A list containing a tensor of face detections for each image in\n",
        "            the batch. If no faces are found for an image, returns a tensor\n",
        "            of shape (0, 17).\n",
        "\n",
        "        Each face detection is a PyTorch tensor consisting of 17 numbers:\n",
        "            - ymin, xmin, ymax, xmax\n",
        "            - x,y-coordinates for the 6 keypoints\n",
        "            - confidence score\n",
        "        \"\"\"\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.from_numpy(x).permute((0, 3, 1, 2))\n",
        "\n",
        "        assert x.shape[1] == 3\n",
        "        assert x.shape[2] == 192\n",
        "        assert x.shape[3] == 192\n",
        "\n",
        "        # 1. Preprocess the images into tensors:\n",
        "        x = x.to(self._device())\n",
        "        x = self._preprocess(x)\n",
        "\n",
        "        # 2. Run the neural network:\n",
        "        with torch.no_grad():\n",
        "            out = self.__call__(x)\n",
        "\n",
        "        # 3. Postprocess the raw predictions:\n",
        "        detections, confidences = out\n",
        "        detections[0:-1:3] *= self.x_scale\n",
        "        detections[1:-1:3] *= self.y_scale\n",
        "\n",
        "        return detections.view(-1, 3), confidences\n"
      ],
      "metadata": {
        "id": "nNMaqurYp1di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# Training\n",
        "#####################################\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "KcJfjdXwrg6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "# class WingLoss(nn.Module):\n",
        "#     def __init__(self, width=5, curvature=0.5):\n",
        "#         super(WingLoss, self).__init__()\n",
        "#         self.width = width\n",
        "#         self.curvature = curvature\n",
        "#         self.C = self.width - self.width * np.log(1 + self.width / self.curvature)\n",
        "#\n",
        "#     def forward(self, prediction, target):\n",
        "#         diff = target - prediction\n",
        "#         diff_abs = diff.abs()\n",
        "#         loss = diff_abs.clone()\n",
        "#\n",
        "#         idx_smaller = diff_abs < self.width\n",
        "#         idx_bigger = diff_abs >= self.width\n",
        "#\n",
        "#         loss[idx_smaller] = self.width * torch.log(1 + diff_abs[idx_smaller] / self.curvature)\n",
        "#         loss[idx_bigger]  = loss[idx_bigger] - self.C\n",
        "#         loss = loss.mean()\n",
        "#         return loss\n",
        "#\n",
        "\n",
        "# criterion = WingLoss()\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "jvfIbyMfJF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "root_dir = os.getcwd()   #root directory of project\n",
        "lr = 10e-4   # learning rate\n",
        "epoch_n = 20   #number of training epochs\n",
        "image_size = 192   #input image-mask size\n",
        "batch_size = 64    #training batch size\n",
        "\n",
        "model = FaceMesh().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n",
        "\n",
        "# training configurations\n",
        "trainset = LoadMixedData(size = image_size, train = True, train_test_split = 0.8)\n",
        "trainloader = DataLoader(trainset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "testset = LoadMixedData(size = image_size, train = False, train_test_split = 0.8)\n",
        "testloader = DataLoader(testset, batch_size = batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "jq8tSrgGJBf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use checkpoint model for training\n",
        "load = True\n",
        "if load:\n",
        "    print('loading model')\n",
        "    cpt = torch.load('/content/drive/MyDrive/Colab Notebooks/model/model_checkpoint.pth')\n",
        "    model.load_state_dict(cpt['model_state_dict'])\n",
        "    #optimizer.load_state_dict(cpt['optimizer_state_dict'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.00001)\n",
        "\n",
        "start_time = datetime.now()\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "for e in range(epoch_n):\n",
        "\n",
        "    print(\"######## Train ########\")\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for data in trainloader:\n",
        "        image, label = data\n",
        "        #print(\"==========DEBUG DATALOADER============\")\n",
        "        #plt.imshow(image[0,:,:,:].permute(1,2,0))\n",
        "        #plt.scatter(label[0,0,:], label[0,1,:])\n",
        "        #plt.savefig(\"dataloader_inspect.png\")\n",
        "        #break\n",
        "        image = image.float().to(device)  # (b x 3 x 192 x 192)\n",
        "        label = label.float().to(device)  # (b x 2 x 68)\n",
        "\n",
        "        output, confidence = model(image) # output: (b, 204), confidence: (b, 1)\n",
        "        loss = criterion(output.view(batch_size, 3, -1)[:, :2, :], label[:, :2, :])\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print('Epoch %d / %d --- Loss: %.4f' % (e + 1, epoch_n, epoch_loss / trainset.__len__()))\n",
        "    print(datetime.now())\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "    print(\"######## Validation ########\")\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(testloader):\n",
        "            image, label = data\n",
        "\n",
        "            image = image.float().to(device)\n",
        "            label = label.float().to(device)\n",
        "\n",
        "            pred, confidence = model(image)\n",
        "            loss = criterion(pred.view(batch_size, 3, -1)[:, :2, :], label[:, :2, :])\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, pred_labels = torch.max(pred, dim = 1)\n",
        "\n",
        "        print('Loss: %.4f' % (total_loss / testset.__len__()))\n",
        "        valid_loss.append(total_loss / testset.__len__())\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': e,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'valid_loss': valid_loss,\n",
        "        }, \"/content/drive/MyDrive/Colab Notebooks/model/model_checkpoint.pth\")\n",
        "\n",
        "end_time = datetime.now()\n",
        "\n",
        "\n",
        "end_time = datetime.now()\n",
        "delta = end_time - start_time\n",
        "s = delta.total_seconds()\n",
        "m, s = divmod(s, 60)\n",
        "h, m = divmod(m, 60)\n",
        "print(f\"Start: {datetime.strftime(start_time, '%H:%M:%S')}\")\n",
        "print(f\"End: {datetime.strftime(end_time, '%H:%M:%S')}\")\n",
        "print(f\"Training time: {int(h)} h {int(m)} min {int(s)} s\")\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(6,2))\n",
        "ep = list(range(len(train_loss)))\n",
        "ax1 = plt.subplot(111)\n",
        "plt.plot(ep, train_loss, 'r', label=\"Train loss\")\n",
        "plt.plot(ep, valid_loss, 'b', label=\"Validation loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"Loss.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e4ULly5xGlD",
        "outputId": "c9107e2d-50fe-48b1-f69f-5aa5a5d907e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model\n",
            "######## Train ########\n",
            "Epoch 1 / 20 --- Loss: 4.3431\n",
            "2023-12-27 05:50:04.494449\n",
            "######## Validation ########\n",
            "Loss: 3.7080\n",
            "######## Train ########\n",
            "Epoch 2 / 20 --- Loss: 3.6722\n",
            "2023-12-27 05:56:37.712963\n",
            "######## Validation ########\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ub1T_6pkzmJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}